---
title: "Report I"
author: "Anna Szymanek (230042), Patryk Wielopolski (234891)"
output:
  pdf_document:
    number_sections: True
header-includes:
- \usepackage{amsmath}
- \usepackage{float}
- \usepackage{listings}
- \floatplacement{figure}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.align='center', message=F, warning=F) 
```

```{r}
# LaTeX packages
library(knitr)
library(xtable)

options(xtable.comment = FALSE)

# Data analysis
library(tidyverse)

# Plots
library(ggplot2)
library(patchwork)
library(latex2exp)

# Dimensionality reduction
library(fastICA)
library(NMF)

df <- read_csv2('data/bank-additional-full.csv')
df[, 'y'] <- as.factor(df$y)

df <- df %>% mutate(
  pdays = na_if(pdays, 999)
)
```

```{r}
df_new_clients <- df %>% 
  filter(is.na(pdays)) %>% 
  filter(poutcome == 'nonexistent') %>% 
  filter(previous == 0) %>% 
  dplyr::select(-pdays, -poutcome, -previous, -duration, -nr.employed)

df_new_clients <- df_new_clients %>% 
  dplyr::select(-default)

df_new_clients <- df_new_clients %>% 
  filter(marital != 'unknown')

df_new_clients <- df_new_clients %>% 
  mutate(education = case_when(
    str_detect(education, 'basic') ~ 'basic',
    str_detect(education, 'illiterate') ~ 'unknown',
    TRUE ~ education
  ))

df_new_clients <- df_new_clients %>% 
  filter(housing != 'unknown') %>% 
  mutate(housing = if_else(housing == 'yes', 1, 0))

df_new_clients <- df_new_clients %>% 
  mutate(loan = if_else(loan == 'yes', 1, 0))

df_new_clients <- df_new_clients %>% 
  mutate(contact_cellular = if_else(contact == 'cellular', 1, 0)) %>% 
  dplyr::select(-contact)

df_new_clients <- df_new_clients %>% 
  dplyr::select(-month)

df_new_clients <- df_new_clients %>% 
  dplyr::select(-day_of_week)

# View(df_new_clients)
```

\section{Introduction}
In this report we will continue the analysis of the dataset \cite{dataset} connected to direct marketing campaigns of a Portuguese banking institution. This time we will explore dimension reduction techniques and cluster analysis.

We will begin our analytical journey with dimension reduction techniques during which we would like to better understand our dataset via visualization and extract useful features for classification and clustering. During exploration of the second task we would like to reveal the hidden structure of the data. Moreover we would like to find out how it is related to the response variable \textit{y} - information whether bank's product (term deposit) would be subscribed or not by given client. Furthermore we would like to find dependencies between clients in our dataset and identify some specific group of clients. Finally, we would like to utilize results of the dimension reduction techniques in the classification task and compare our results with previously obtained in the first report.

\section{Methods}
\label{sec:methods}
In the following sections we will go through all mentioned in introduction tasks - dimension reduction, classification and cluster analysis. We will use all the transformations used in the first part of the project in context of the classification, i.e. we will focus only on the new clients who has never been targeted in previous campaigns and additionaly we performed data transformations connected to missing data, rare values and categorical variables encoding.

\subsection{Dimension reduction}
In this section we will go through a few dimension reduction techniques in order to visualize our dataset from different perspectives and look for interesting patterns which we hope to utilize in next section connected to classification and cluster analysis. We will use and compare following methods:
\begin{itemize}
  \item Principal component analysis (PCA),
  \item Multidimensional scaling (MDS),
  \item Independent component analysis (ICA),
  \item Non-negative matrix factorization (NMF).
 \end{itemize}
 
\subsubsection{Principal components analysis}
We will begin with principal components analysis. Firstly we will try naive approach and take whole dataset, conduct one hot encoding for categorical variables, remove target variable and analyse the obtained results in context of target variable. Moreover we will only center our data without scaling and treat it as a experiment how does scaling influence the results. We already known from the first report that socio-economic variables has huge values compared to the other variables and we expect that results may be higlhy influenced by these variables.

\begin{figure}
```{r fig.width=8, fig.height=4}
results_pca <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  DataExplorer::dummify() %>% 
  prcomp(scale. = FALSE, center = TRUE)

results_pca_df <- data.frame(results_pca$x)
results_pca_df['y'] <- df_new_clients['y']
results_pca_df %>% 
  ggplot(aes(x = PC1, y = PC2, color = y)) +
  geom_point(size = 1)
```
\caption{Results of the Principal component analysis without scaling in the context of \textit{y} variable.}
\label{fig:pca-all-data-no-scaling}
\end{figure}

The results of the PCA without scaling we can observe on Figure \ref{fig:pca-all-data-no-scaling}. As we expected the principal components have big values which are probably influenced by socio-economic variables. Let's explore the formulated cluster with \textit{PC1 < -80000} and \textit{PC2 > 0} values and find out what data is in this subspace.

```{r results='asis'}
indices <- results_pca_df %>% 
  mutate(n = 1:n()) %>% 
  filter(PC1 < -80000, PC2 > 0) %>% 
  pull(n)

df_new_clients[indices, ] %>% 
  select(emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m) %>% 
  head(20) %>% 
  xtable(
    caption = "Example data from PCA's (without scaling) one cluster.",
    label = "table:pca-cluster-example"
  )
```

We can observe part of the results in the Table \ref{table:pca-cluster-example}. We only present a small subset of the extracted cluster however we can easily observe that the socio-economic values were indicating character of this group. We expect that rest of the formulated clusters have a similar structure. In context of the extracting knowledge it may be very interesting result because it's possible to extract some correlated periods in economics however in context of clients clustering or term deposit subscription it's not a direction we want to follow (because we cannot see any particular structure in \textit{y} variable). Let's explore if something will change when we scale our dataset.

\begin{figure}
```{r fig.width=8, fig.height=4}
results_pca <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  DataExplorer::dummify() %>% 
  prcomp(scale. = TRUE, center = TRUE)

results_pca_df <- data.frame(results_pca$x)
results_pca_df['y'] <- df_new_clients['y']
results_pca_df %>% 
  ggplot(aes(x = PC1, y = PC2, color = y)) +
  geom_point(size = 1)
```
\caption{Results of the Principal component analysis with scaling in the context of \textit{y} variable.}
\label{fig:pca-all-data-scaling}
\end{figure}

We can observe obtained results on the Figure \ref{fig:pca-all-data-scaling}. We can distinguish some area of the plot which is mostly covered by \textit{no} response - lower and upper part of the plot, and middle one with the advantage of \textit{yes} response. That's a very good information in context of our classification task where such a structure may be very helpful for a model. Let's explore more deeply these results and find out what number of the components should be optimal and what variables are in these components.

\begin{figure}
```{r fig.width=8, fig.height=4}
component <- sapply(1:29, function(i) {paste(i, sep = "")})
variance <- (results_pca$sdev ^2)/sum(results_pca$sdev^2)
cumulative.variance <- cumsum(variance)

variances <- data.frame(
  component = factor(component, levels = component),
  variance = variance,
  cumuluative.variance = cumulative.variance
)

variances %>% 
  ggplot(aes(x = component, y = cumulative.variance)) +
  geom_bar(stat = 'identity') +
  geom_hline(yintercept = 0.8, color = 'red')
```
\caption{Cummulative variance plot for PCA with data scaling.}
\label{fig:pca-variance}
\end{figure}

We can observe on Figure \ref{fig:pca-variance} that our first component explains only around 10% of the variance. Moreover we require first seventeen component to explain more than 80% of variance. That could potentialy explain why we don't have such a clear groups in the 2D plot. Based on these observations we will take these components for classification task.

```{r results='asis'}
results_pca$rotation[, 1] %>% abs() %>% sort(decreasing = TRUE) %>% data.frame() %>% head(10) %>% 
  xtable(
    label = 'tab:PCA-first-component',
    caption = 'Top 10 most influential variables in the first component of PCA with scaling.'
  )
```

Let's also take a look into Table \ref{tab:PCA-first-component} where we have information about content of the first PCA's component. It contains mostly information about basic/univesristy education, marital status and blue collar / administration job.

So far we have prepared PCA for classification task and now we will focus on PCA in context of clusters analysis. In this case we would like to analyse information only about clients so we will use only variables connected to them and skip variables connect to socio-economic factors. The resulting variables are as follows:
 \begin{itemize}
  \item age,
  \item job,
  \item marital
  \item education,
  \item housing,
  \item loan.
\end{itemize}
We've tested the different combinations of PCA - with / without scaling and with / without age variable (which is the only one numeric variable). The most interesting results we've obtained for PCA without scaling and without age variable. The results are presented on Figure \ref{fig:pca-clusters}.
 
\begin{figure}
```{r}
results_pca <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  dplyr::select(job, marital, education, housing, loan) %>% 
  DataExplorer::dummify() %>% 
  prcomp(scale. = FALSE, center = TRUE)

results_pca_df <- data.frame(results_pca$x)
results_pca_df['y'] <- df_new_clients['y']
results_pca_df %>% 
  ggplot(aes(x = PC1, y = PC2, color = y)) +
  geom_point(size = 1)
```
\caption{Results of the Principal component analysis in the context of \textit{y} variable using age, job, marital, education, housing, loan variables.}
\label{fig:pca-clusters}
\end{figure}

As we can observe we've obtained clearly separable clusters for only 2D results which is very interesting and may be very useful for clustering task. Similarily to the previous PCA analysis let's explore one cluster, for example \textit{PC1 < -1}.

```{r results='asis'}
indices <- results_pca_df %>% 
  mutate(n = 1:n()) %>% 
  filter(PC1 < -1) %>% 
  pull(n)

df_new_clients[indices, ] %>% 
  select(job, marital, education, housing, loan) %>% 
  head(20) %>% 
  xtable(
    caption = "Example data from PCA's for cluster analysis one cluster.",
    label = "table:pca-ca-cluster-example"
  )
```

As we can observe in the Table \ref{table:pca-ca-cluster-example} we have found cluster of people who work as a blue collars, are married and have basic education. It's very interesting result as we will be able to found very homogenous group of people.

\begin{figure}
```{r}
biplot(results_pca)
```
\caption{Biplot of the Principal component analysis using job, marital, education, housing, loan variables.}
\label{fig:pca-biplot}
\end{figure}

Let's explore a little bit more this PCA resutls and observe Figure \ref{fig:pca-biplot}. We can cleary see that on the left upper corner we've got married people and on the other side we've got respectively divorced and single people which in general create group of the single people. On the opposite diagonal we have got encoded information about job and education. On the lower left corenr we've got correlated blue collar jobs and basic education and on the opposite side we've got administration jobs with university education. To our mind this is very interesting and exciting results as it was possible to so clearly distinguish groups of people.

```{r}
results_pca_df['job'] <- df_new_clients['job']
results_pca_df['marital'] <- df_new_clients['marital']
results_pca_df['education'] <- df_new_clients['education']
```

\begin{figure}
```{r fig.height=8, fig.width=5}
plot_marital <- results_pca_df %>% 
  ggplot(aes(x = PC1, y = PC2, color = marital)) +
  geom_point(size = 1)

plot_job <- results_pca_df %>% 
  ggplot(aes(x = PC1, y = PC2, color = job)) +
  geom_point(size = 1)

plot_education <- results_pca_df %>% 
  ggplot(aes(x = PC1, y = PC2, color = education)) +
  geom_point(size = 1)

plot_marital / plot_job / plot_education
```
\caption{Results of the Principal component analysis in the context of \text{marital}, \textit{job}, \textit{education} variable using age, job, marital, education, housing, loan variables.}
\label{fig:pca-ca-analysis}
\end{figure}

We checked these hypothesis on Figure \ref{fig:pca-ca-analysis}. We can clearly confirm our observation about marital status and seperate groups between three formulated lines. In the context of job we can also confirm that we have separate groups of administration and blue collar jobs. Moreover we can observe some kind of the structure in center clusters. At the last part of this plot we can observe some structure in education where on the bottom part we've got people with basic education, in the middle mixed but with majority of high school and professional courses, lastly we have at the top people with university degree.

At this point we will end up our PCA analysis. We've performed extensive analysis with very interesting results which will be definitely utilized during classification and cluster analysis. In context of the cluster analysis we could even say that we've partialy performed it with very satisfactory results.

\subsubsection{Multidimensional scaling}
In this section we will test another dimension reduction technique - multidimensional scaling. At this moment we recall that our dataset has `r nrow(df_new_clients)` rows. It's important in this moment because this method uses similarity matrix which has to compare all rows to all rows. In our case it would produce very big matrix which may be not possible to handle. Because of that fact we will use randomly selected 15% of our dataset with stratification on \textit{y} variable.

```{r}
df_new_client_yes <- df_new_clients %>% 
  filter(y == 'yes') %>% 
  sample_frac(0.15)

df_new_client_no <- df_new_clients %>% 
  filter(y == 'no') %>% 
  sample_frac(0.15)

df_new_clients_sample <- rbind.data.frame(df_new_client_no, df_new_client_yes)
```

```{r cache=TRUE}
dissimilarities <- df_new_clients_sample %>% 
  dplyr::select(-y) %>% 
  mutate(
    job = factor(job),
    marital = factor(marital),
    education = factor(education),
    loan = factor(loan), 
    campaign = factor(campaign),
    contact_cellular = factor(contact_cellular)
  ) %>% 
  daisy() %>% 
  as.matrix()

mds <- cmdscale(dissimilarities)
```

\begin{figure}
```{r}
mds.df <- mds %>% data.frame()
mds.df <- bind_cols(mds.df, df_new_clients_sample)

mds.df %>% 
  ggplot(aes(x = X1, y = X2, color = y)) +
  geom_point()
```
\caption{Results of the Multidimensional scaling in context of \textit{y} variable.}
\label{fig:mds}
\end{figure}

We've performed dissimilarity calculation using gower metric without standarization (using standarization doesn't change anything) using all variables except \textit{y} and then used multidimensional scaling with rank equal 2. The results can be found on Figure \ref{fig:mds}. We can easily see two disjoint groups however unfortunately without clear separation on \textit{y}. Let's find out by which variables our transformed data was separated.

\begin{figure}
```{r fig.height=8, fig.width=5}
plot_housing <- mds.df %>% 
  ggplot(aes(x = X1, y = X2, color = housing)) +
  geom_point()

plot_loan <- mds.df %>% 
  ggplot(aes(x = X1, y = X2, color = loan)) +
  geom_point()

plot_euribor <- mds.df %>% 
  ggplot(aes(x = X1, y = X2, color = euribor3m)) +
  geom_point()

plot_housing / plot_loan / plot_euribor
```
\caption{Results of the Multidimensional scaling in the context of \text{housing}, \textit{loan}, \textit{euribor3m} variables.}
\label{fig:mds-housing-loan-euribor}
\end{figure}

We conclude from Figure \ref{fig:mds-housing-loan-euribor} that main splitting factor was housing. The other two the most interesting variables in our opinion was loan and euribor. Separation and specific areas of subgroups for the other variables also could be seen however not so clearly. Similarily to PCA example let's explore only client specific data.


```{r cache=TRUE}
dissimilarities_client <- df_new_clients_sample %>% 
  dplyr::select(-y) %>% 
  mutate(
    job = factor(job),
    marital = factor(marital),
    education = factor(education),
    loan = factor(loan), 
    campaign = factor(campaign)
  ) %>% 
  dplyr::select(job, marital, education, housing, loan) %>% 
  daisy() %>% 
  as.matrix()

mds_client <- cmdscale(dissimilarities_client)
```

\begin{figure}
```{r}
mds_client.df <- mds_client %>% data.frame()
mds_client.df <- bind_cols(mds_client.df, df_new_clients_sample)

mds_client.df %>% 
  ggplot(aes(x = X1, y = X2, color = y)) +
  geom_point()
```
\caption{Results of the Multidimensional scaling in context of \textit{y} variable for only client data.}
\label{fig:mds-client}
\end{figure}

We can observe results of the MDS in context of only client data on Figure \ref{fig:mds-client}. Also here we can find some separate groups. However this time we also cannot see any particular pattern in context of target variable. In next step we will try to find which variables separates groups.

\begin{figure}
```{r}
mds_client.df %>% 
  mutate(
    housing = factor(housing),
    loan = factor(loan)
  ) %>% 
  ggplot(aes(x = X1, y = X2, color = housing, shape = loan)) +
  geom_point(size = 2, alpha = 0.7) + 
  scale_shape_manual(values = c(0, 4))
```
\caption{Results of the Multidimensional scaling in context of \textit{housing} and \textit{loan} variables for only client data.}
\label{fig:mds-client-separate}
\end{figure}

After quick search we've obtained the following results which are presented also on Figure \ref{fig:mds-client-separate}. Our main separating varialbe is \textit{housing} and in the group of clients which have a house we can also distinguish people with and without loan. It's quite interesting pattern because it's very natural separation in real-world as people can have house on credit / on their own or doesn't have a house and may or may not have a loan.

Concluding, multidimensional scaling gave us also very interesting results, especially in context of data visualization / understanding and grouping. It also may be very powerful tool for us in the next sections connected to classification and cluster analysis.

\subsubsection{FastICA}
```{r}
results_ica <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  dplyr::select(age, job, marital, education, housing, loan) %>% 
  DataExplorer::dummify() %>% 
  scale() %>% 
  fastICA(n.comp = 2)
```
```{r}
results_ica_s <- data.frame(results_ica$S)
results_ica_s['y'] <- df_new_clients['y']

ggplot(results_ica_s, aes(x = X1, y = X2, color = y)) + 
  geom_point(size = 1)
```
Same as PCA

\subsubsection{Non-negative matrix factorization}

```{r}
results_nmf <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  dplyr::select(job, marital, education, housing, loan) %>% 
  DataExplorer::dummify() %>% 
  nmf(rank = 2)

results_nmf_W <- data.frame(results_nmf@fit@W)
results_nmf_W['y'] <- df_new_clients['y']

fig_base <- ggplot(results_nmf_W, aes(x = X1, y = X2, color = y)) +
  geom_point() +
  ggtitle('NMF without age variable') +
  theme(legend.position = "none")

results_nmf <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  dplyr::select(age, job, marital, education, housing, loan) %>% 
  DataExplorer::dummify() %>% 
  nmf(rank = 2)

results_nmf_W <- data.frame(results_nmf@fit@W)
results_nmf_W['y'] <- df_new_clients['y']

fig_age <- ggplot(results_nmf_W, aes(x = X1, y = X2, color = y)) +
  geom_point() +
  ggtitle('NMF with age variable')

fig_base | fig_age
```


\subsection{Classification}
Similar to the 1. project

\subsection{Clustering}
Objective: group objects according to their similarity
Methods: 
 - k-means
 - Partition Around Medoids (PAM)
 - Aglomerative Nesting (AGNES) 
 - Other (HDBSCAN?)
Quality assessment of cluster analysis results:
 - Average silhouette vs different K
 - Separation / Compactness / Connectedness
 
 Ideas:
  - Use Custering LARge Applications (CLARA) instead PAM if computations are slow
  - Different dissimilarity measures - Gower distance
  - Analyse what are the characteristic properties of objects thatwere assign to a given cluster (for example, in individualclusters you can analyse: average values for numerical featuresand counts for qualitative features).
  - Dendogram and banner plot for AGNES
  - Single / Complete / Average linkage for AGNES
  - Dunn index for cluster results analysis

Cluster results analysis:  
 - Internal validation:Use the averagesilhouette indexto comparethe results obtained for different clustering algorithms (e.g. PAMand AGNES) and for a different number of clustersK. Try to decideon the optimal number of clusters.
 - External validation:Use simple contingency table (confusionmatrix) to compare clustering results with real class membership. Compare results for different clustering algorithms, includingpartitioning and hierarchical methods. (Hint: you can use functionmatchClasses{e1071} to find anoptimal assignment (mapping) between two set of labels).
  
  Notes:
   - AGNES will probably require data sampling
  
\section{Results and discussion}
\label{sec:results}


\bibliographystyle{unsrt}
\bibliography{references}
