---
title: "Report I"
author: "Anna Szymanek (230042), Patryk Wielopolski (234891)"
output:
  pdf_document:
    number_sections: True
header-includes:
- \usepackage{amsmath}
- \usepackage{float}
- \usepackage{listings}
- \floatplacement{figure}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.align='center', message=F, warning=F) 
```

```{r}
# LaTeX packages
library(knitr)
library(xtable)

options(xtable.comment = FALSE)

# Data analysis
library(tidyverse)

# Plots
library(ggplot2)
library(patchwork)
library(latex2exp)

# Dimensionality reduction
library(fastICA)
library(NMF)

COLOR_1 <- '#18729E'
COLOR_2 <- '#EAC9A2'

df <- read_csv2('data/bank-additional-full.csv')
df[, 'y'] <- as.factor(df$y)

df <- df %>% mutate(
  pdays = na_if(pdays, 999)
)
```

```{r}
df_new_clients <- df %>% 
  filter(is.na(pdays)) %>% 
  filter(poutcome == 'nonexistent') %>% 
  filter(previous == 0) %>% 
  dplyr::select(-pdays, -poutcome, -previous, -duration, -nr.employed)

df_new_clients <- df_new_clients %>% 
  dplyr::select(-default)

df_new_clients <- df_new_clients %>% 
  filter(marital != 'unknown')

df_new_clients <- df_new_clients %>% 
  mutate(education = case_when(
    str_detect(education, 'basic') ~ 'basic',
    str_detect(education, 'illiterate') ~ 'unknown',
    TRUE ~ education
  ))

df_new_clients <- df_new_clients %>% 
  filter(housing != 'unknown') %>% 
  mutate(housing = if_else(housing == 'yes', 1, 0))

df_new_clients <- df_new_clients %>% 
  mutate(loan = if_else(loan == 'yes', 1, 0))

df_new_clients <- df_new_clients %>% 
  mutate(contact_cellular = if_else(contact == 'cellular', 1, 0)) %>% 
  dplyr::select(-contact)

df_new_clients <- df_new_clients %>% 
  dplyr::select(-month)

df_new_clients <- df_new_clients %>% 
  dplyr::select(-day_of_week)

# View(df_new_clients)
```

\section{Introduction}
In this report we will continue the analysis of the dataset \cite{dataset} connected to direct marketing campaigns of a Portuguese banking institution. This time we will explore dimension reduction techniques and cluster analysis.

We will begin our analytical journey with dimension reduction techniques during which we would like to better understand our dataset via visualization and extract useful features for classification and clustering. During exploration of the second one we would like to reveal the hidden structure of the data. Moreover we would like to find out how it is related to the response variable \textit{y} - information whether bank's product (term deposit) would be subscribed or not by given client. Furthermore we would like to find dependencies between clients in our dataset and identify some specific group of clients. Finally, we would like to utilize results of the dimension reduction techniques in the classification task and compare our results with previously obtained in the first report.

```{r}
# Cluster analysis with quality assessment
# Application of the selected dimension reduction method in connection with classification and cluster analysis

# Customer segmentation
```

\section{Methods}
\label{sec:methods}

We will use all the transformations used in the first part of the project.

In the following sections we through dimenstion reduction techniques where we will visualize date and prepare data to the next tasks - classification and cluster analysis.

\subsection{Dimension reduction}
In this section we will go through a few dimension reduction techniques in order to visualize our dataset from different perspectives and look for interesting patterns which we hope to utilize in next section connected to classification and cluster analysis. We will use and compare following methods:
\begin{itemize}
  \item Principal component analysis (PCA),
  \item Multidimensional scaling (MDS),
  \item Independent component analysis (ICA),
  \item Non-negative matrix factorization (NMF).
 \end{itemize}
 
\subsubsection{Principal components analysis}
We will begin with principal components analysis. As a first try we will take whole dataset, conduct one hot encoding for categorical variables, remove target variable and analyse the obtained results in context of target variable. As a first try we will only center our data withou scaling however we already known from the first report that socio-economic variables has huge values compared to the other variables and we expect that results may be higlhy influenced by these variables.

\begin{figure}
```{r fig.width=8, fig.height=4}
results_pca <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  DataExplorer::dummify() %>% 
  prcomp(scale. = FALSE, center = TRUE)

results_pca_df <- data.frame(results_pca$x)
results_pca_df['y'] <- df_new_clients['y']
results_pca_df %>% 
  ggplot(aes(x = PC1, y = PC2, color = y)) +
  geom_point(size = 1)
```
\caption{Results of the Principal component analysis without scaling in the context of \textit{y} variable.}
\label{fig:pca-all-data-no-scaling}
\end{figure}

The results of the PCA without scaling we can observe on Figure \ref{fig:pca-all-data-no-scaling}. As we expected the principal components have big values which are probably influenced by socio-economic variables. Let's explore the formulated cluster with \textit{PC1 < -80000} and \textit{PC2 > 0} values and find out what data is this subspace.

```{r results=asis}
indices <- results_pca_df %>% 
  mutate(n = 1:n()) %>% 
  filter(PC1 < -80000, PC2 > 0) %>% 
  pull(n)

df_new_clients[indices, ] %>% 
  select(emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m) %>% 
  head(20) %>% 
  kable(
    caption = "Example data from PCA's (without scaling) one cluster.",
    label = "table:pca-cluster-example"
  )
```

We can observe part of the results in the Table \ref{table:pca-cluster-example}. We only present a small subset of the extracted cluster however we can easily observe that the socio-economic values were indicating character of this group. We expect that rest of the formulated clusters have a similar structure. In context of the extracting knowledge it may be very interesting result because it's possible to extract some correlated periods in economics however in context of clients clustering or term deposit subscription (because we cannot see any particular structure in \textit{y} variable) it's not a direction we want to follow. Let's explore if something will change when we scale our dataset.

\begin{figure}
```{r fig.width=8, fig.height=4}
results_pca <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  DataExplorer::dummify() %>% 
  prcomp(scale. = TRUE, center = TRUE)

results_pca_df <- data.frame(results_pca$x)
results_pca_df['y'] <- df_new_clients['y']
results_pca_df %>% 
  ggplot(aes(x = PC1, y = PC2, color = y)) +
  geom_point(size = 1)
```
\caption{Results of the Principal component analysis with scaling in the context of \textit{y} variable.}
\label{fig:pca-all-data-scaling}
\end{figure}

We can observe obtained results on the Figure \ref{fig:pca-all-data-scaling}. We can distinguish some area of the plot which is mostly covered by \textit{no} response - lower and upper part of the plot, and middle one with the advantage of \textit{yes} response. That's a very good information in context of our classification task where such a structure may be very helpful for a model.

However for clusters analysis we would like to analyse information only about clients so we will use only variables connected to them and skip variables connect to socio-economic factors. The resulting variables are as follows:
 \begin{itemize}
  \item age,
  \item job,
  \item marital
  \item education,
  \item housing,
  \item loan.
 
\begin{figure}
```{r}
results_pca <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  dplyr::select(age, job, marital, education, housing, loan) %>% 
  DataExplorer::dummify() %>% 
  prcomp(scale. = TRUE, center = TRUE)

results_pca_df <- data.frame(results_pca$x)
results_pca_df['y'] <- df_new_clients['y']
results_pca_df %>% 
  ggplot(aes(x = PC1, y = PC2, color = y)) +
  geom_point(size = 1)
```
\caption{Results of the Principal component analysis in the context of \textit{y} variable using age, job, marital, education, housing, loan variables.}
\label{fig:pca-clusters}
\end{figure}

```{r}
biplot(results_pca)
```


The results we can observe on the Figure \ref{fig:pca-clusters}. We can 

\subsubsection{Multidimensional scalling}

Not possible to calculate ... as they need to calculate all comparisons between ~34000 samples ? Maybe resampling ?

\subsubsection{FastICA}
```{r}
results_ica <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  dplyr::select(age, job, marital, education, housing, loan) %>% 
  DataExplorer::dummify() %>% 
  scale() %>% 
  fastICA(n.comp = 2)
```
```{r}
results_ica_s <- data.frame(results_ica$S)
results_ica_s['y'] <- df_new_clients['y']

ggplot(results_ica_s, aes(x = X1, y = X2, color = y)) + 
  geom_point(size = 1)
```
Same as PCA

\subsubsection{Non-negative matrix factorization}

```{r}
results_nmf <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  dplyr::select(job, marital, education, housing, loan) %>% 
  DataExplorer::dummify() %>% 
  nmf(rank = 2)

results_nmf_W <- data.frame(results_nmf@fit@W)
results_nmf_W['y'] <- df_new_clients['y']

fig_base <- ggplot(results_nmf_W, aes(x = X1, y = X2, color = y)) +
  geom_point() +
  ggtitle('NMF without age variable') +
  theme(legend.position = "none")

results_nmf <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  dplyr::select(age, job, marital, education, housing, loan) %>% 
  DataExplorer::dummify() %>% 
  nmf(rank = 2)

results_nmf_W <- data.frame(results_nmf@fit@W)
results_nmf_W['y'] <- df_new_clients['y']

fig_age <- ggplot(results_nmf_W, aes(x = X1, y = X2, color = y)) +
  geom_point() +
  ggtitle('NMF with age variable')

fig_base | fig_age
```


\subsection{Classification}
Similar to the 1. project

\subsection{Clustering}
Objective: group objects according to their similarity
Methods: 
 - k-means
 - Partition Around Medoids (PAM)
 - Aglomerative Nesting (AGNES) 
 - Other (HDBSCAN?)
Quality assessment of cluster analysis results:
 - Average silhouette vs different K
 - Separation / Compactness / Connectedness
 
 Ideas:
  - Use Custering LARge Applications (CLARA) instead PAM if computations are slow
  - Different dissimilarity measures - Gower distance
  - Analyse what are the characteristic properties of objects thatwere assign to a given cluster (for example, in individualclusters you can analyse: average values for numerical featuresand counts for qualitative features).
  - Dendogram and banner plot for AGNES
  - Single / Complete / Average linkage for AGNES
  - Dunn index for cluster results analysis

Cluster results analysis:  
 - Internal validation:Use the averagesilhouette indexto comparethe results obtained for different clustering algorithms (e.g. PAMand AGNES) and for a different number of clustersK. Try to decideon the optimal number of clusters.
 - External validation:Use simple contingency table (confusionmatrix) to compare clustering results with real class membership. Compare results for different clustering algorithms, includingpartitioning and hierarchical methods. (Hint: you can use functionmatchClasses{e1071} to find anoptimal assignment (mapping) between two set of labels).
  
  Notes:
   - AGNES will probably require data sampling
  
\section{Results and discussion}
\label{sec:results}


\bibliographystyle{unsrt}
\bibliography{references}
