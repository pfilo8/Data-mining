---
title: "Report I"
author: "Anna Szymanek (230042), Patryk Wielopolski (234891)"
output:
  pdf_document:
    number_sections: True
header-includes:
- \usepackage{amsmath}
- \usepackage{float}
- \usepackage{listings}
- \floatplacement{figure}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(fig.align='center', message=F, warning=F) 
```

```{r}
# LaTeX packages
library(knitr)
library(xtable)

options(xtable.comment = FALSE)

# Data analysis
library(tidyverse)

# Plots
library(ggplot2)
library(patchwork)
library(latex2exp)

# Dimensionality reduction
library(fastICA)
library(NMF)

# Models
library(mlr3)
library(mlr3learners)
library(mlr3pipelines)
library(kknn)
library(MASS)
library(ranger)

# Clustering
library(factoextra)
library(cluster)
library(NbClust)

df <- read_csv2('data/bank-additional-full.csv')
df[, 'y'] <- as.factor(df$y)

df <- df %>% mutate(
  pdays = na_if(pdays, 999)
)

set.seed(42)
```

```{r}
df_new_clients <- df %>% 
  filter(is.na(pdays)) %>% 
  filter(poutcome == 'nonexistent') %>% 
  filter(previous == 0) %>% 
  dplyr::select(-pdays, -poutcome, -previous, -duration, -nr.employed)

df_new_clients <- df_new_clients %>% 
  dplyr::select(-default)

df_new_clients <- df_new_clients %>% 
  filter(marital != 'unknown')

df_new_clients <- df_new_clients %>% 
  mutate(education = case_when(
    str_detect(education, 'basic') ~ 'basic',
    str_detect(education, 'illiterate') ~ 'unknown',
    TRUE ~ education
  ))

df_new_clients <- df_new_clients %>% 
  filter(housing != 'unknown') %>% 
  mutate(housing = if_else(housing == 'yes', 1, 0))

df_new_clients <- df_new_clients %>% 
  mutate(loan = if_else(loan == 'yes', 1, 0))

df_new_clients <- df_new_clients %>% 
  mutate(contact_cellular = if_else(contact == 'cellular', 1, 0)) %>% 
  dplyr::select(-contact)

df_new_clients <- df_new_clients %>% 
  dplyr::select(-month)

df_new_clients <- df_new_clients %>% 
  dplyr::select(-day_of_week)

# View(df_new_clients)
```

\section{Introduction}
In this report we will continue the analysis of the dataset \cite{dataset} connected to direct marketing campaigns of a Portuguese banking institution. This time we will explore dimension reduction techniques and cluster analysis.

We will begin our analytical journey with dimension reduction techniques during which we would like to better understand our dataset via visualization and extract useful features for classification and clustering. During exploration of the second task we would like to reveal the hidden structure of the data. Moreover we would like to find out how it is related to the response variable \textit{y} - information whether bank's product (term deposit) would be subscribed or not by given client. Furthermore we would like to find dependencies between clients in our dataset and identify some specific group of clients. Finally, we would like to utilize results of the dimension reduction techniques in the classification task and compare our results with previously obtained in the first report.

\section{Methods}
\label{sec:methods}
In the following sections we will go through all mentioned in introduction tasks - dimension reduction, classification and cluster analysis. We will use all the transformations used in the first part of the project in context of the classification, i.e. we will focus only on the new clients who has never been targeted in previous campaigns and additionaly we performed data transformations connected to missing data, rare values and categorical variables encoding.

\subsection{Dimension reduction}
In this section we will go through a few dimension reduction techniques in order to visualize our dataset from different perspectives and look for interesting patterns which we hope to utilize in next section connected to classification and cluster analysis. We will use and compare following methods:
\begin{itemize}
  \item Principal component analysis (PCA),
  \item Multidimensional scaling (MDS).
 \end{itemize}
 
\subsubsection{Principal components analysis}
We will begin with principal components analysis. Firstly we will try naive approach and take whole dataset, conduct one hot encoding for categorical variables, remove target variable and analyse the obtained results in context of target variable. Moreover we will only center our data without scaling and treat it as a experiment how does scaling influence the results. We already known from the first report that socio-economic variables has huge values compared to the other variables and we expect that results may be higlhy influenced by these variables.

\begin{figure}
```{r fig.width=8, fig.height=4}
results_pca <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  DataExplorer::dummify() %>% 
  prcomp(scale. = FALSE, center = TRUE)

results_pca_df <- data.frame(results_pca$x)
results_pca_df['y'] <- df_new_clients['y']
results_pca_df %>% 
  ggplot(aes(x = PC1, y = PC2, color = y)) +
  geom_point(size = 1)
```
\caption{Results of the Principal component analysis without scaling in the context of \textit{y} variable.}
\label{fig:pca-all-data-no-scaling}
\end{figure}

The results of the PCA without scaling we can observe on Figure \ref{fig:pca-all-data-no-scaling}. As we expected the principal components have big values which are probably influenced by socio-economic variables. Let's explore the formulated cluster with \textit{PC1 < -80000} and \textit{PC2 > 0} values and find out what data is in this subspace.

```{r results='asis'}
indices <- results_pca_df %>% 
  mutate(n = 1:n()) %>% 
  filter(PC1 < -80000, PC2 > 0) %>% 
  pull(n)

df_new_clients[indices, ] %>% 
  select(emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m) %>% 
  head(20) %>% 
  xtable(
    caption = "Example data from PCA's (without scaling) one cluster.",
    label = "table:pca-cluster-example"
  )
```

We can observe part of the results in the Table \ref{table:pca-cluster-example}. We only present a small subset of the extracted cluster however we can easily observe that the socio-economic values were indicating character of this group. We expect that rest of the formulated clusters have a similar structure. In context of the extracting knowledge it may be very interesting result because it's possible to extract some correlated periods in economics however in context of clients clustering or term deposit subscription it's not a direction we want to follow (because we cannot see any particular structure in \textit{y} variable). Let's explore if something will change when we scale our dataset.

\begin{figure}
```{r fig.width=8, fig.height=4}
results_pca <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  DataExplorer::dummify() %>% 
  prcomp(scale. = TRUE, center = TRUE)

results_pca_df <- data.frame(results_pca$x)
results_pca_df['y'] <- df_new_clients['y']
results_pca_df %>% 
  ggplot(aes(x = PC1, y = PC2, color = y)) +
  geom_point(size = 1)
```
\caption{Results of the Principal component analysis with scaling in the context of \textit{y} variable.}
\label{fig:pca-all-data-scaling}
\end{figure}

We can observe obtained results on the Figure \ref{fig:pca-all-data-scaling}. We can distinguish some area of the plot which is mostly covered by \textit{no} response - lower and upper part of the plot, and middle one with the advantage of \textit{yes} response. That's a very good information in context of our classification task where such a structure may be very helpful for a model. Let's explore more deeply these results and find out what number of the components should be optimal and what variables are in these components.

\begin{figure}
```{r fig.width=8, fig.height=4}
component <- sapply(1:29, function(i) {paste(i, sep = "")})
variance <- (results_pca$sdev ^2)/sum(results_pca$sdev^2)
cumulative.variance <- cumsum(variance)

variances <- data.frame(
  component = factor(component, levels = component),
  variance = variance,
  cumuluative.variance = cumulative.variance
)

variances %>% 
  ggplot(aes(x = component, y = cumulative.variance)) +
  geom_bar(stat = 'identity') +
  geom_hline(yintercept = 0.8, color = 'red')
```
\caption{Cummulative variance plot for PCA with data scaling.}
\label{fig:pca-variance}
\end{figure}

We can observe on Figure \ref{fig:pca-variance} that our first component explains only around 10% of the variance. Moreover we require first seventeen component to explain more than 80% of variance. That could potentialy explain why we don't have such a clear groups in the 2D plot. Based on these observations we will take these components for classification task.

```{r results='asis'}
results_pca$rotation[, 1] %>% abs() %>% sort(decreasing = TRUE) %>% data.frame() %>% head(10) %>% 
  xtable(
    label = 'tab:PCA-first-component',
    caption = 'Top 10 most influential variables in the first component of PCA with scaling.'
  )
```

Let's also take a look into Table \ref{tab:PCA-first-component} where we have information about content of the first PCA's component. It contains mostly information about basic/univesristy education, marital status and blue collar / administration job.

So far we have prepared PCA for classification task and now we will focus on PCA in context of clusters analysis. In this case we would like to analyse information only about clients so we will use only variables connected to them and skip variables connect to socio-economic factors. The resulting variables are as follows:
 \begin{itemize}
  \item age,
  \item job,
  \item marital
  \item education,
  \item housing,
  \item loan.
\end{itemize}
We've tested the different combinations of PCA - with / without scaling and with / without age variable (which is the only one numeric variable). The most interesting results we've obtained for PCA without scaling and without age variable. The results are presented on Figure \ref{fig:pca-clusters}.
 
\begin{figure}
```{r}
results_pca_client <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  dplyr::select(job, marital, education, housing, loan) %>% 
  DataExplorer::dummify() %>% 
  prcomp(scale. = FALSE, center = TRUE)

results_pca_df_client <- data.frame(results_pca_client$x)
results_pca_df_client['y'] <- df_new_clients['y']
results_pca_df_client %>% 
  ggplot(aes(x = PC1, y = PC2, color = y)) +
  geom_point(size = 1)
```
\caption{Results of the Principal component analysis in the context of \textit{y} variable using age, job, marital, education, housing, loan variables.}
\label{fig:pca-clusters}
\end{figure}

As we can observe we've obtained clearly separable clusters for only 2D results which is very interesting and may be very useful for clustering task. Similarily to the previous PCA analysis let's explore one cluster, for example \textit{PC1 < -1}.

```{r results='asis'}
indices <- results_pca_df %>% 
  mutate(n = 1:n()) %>% 
  filter(PC1 < -1) %>% 
  pull(n)

df_new_clients[indices, ] %>% 
  select(job, marital, education, housing, loan) %>% 
  head(20) %>% 
  xtable(
    caption = "Example data from PCA's for cluster analysis one cluster.",
    label = "table:pca-ca-cluster-example"
  )
```

As we can observe in the Table \ref{table:pca-ca-cluster-example} we have found cluster of people who work as a blue collars, are married and have basic education. It's very interesting result as we will be able to found very homogenous group of people.

\begin{figure}
```{r}
biplot(results_pca)
```
\caption{Biplot of the Principal component analysis using job, marital, education, housing, loan variables.}
\label{fig:pca-biplot}
\end{figure}

Let's explore a little bit more this PCA resutls and observe Figure \ref{fig:pca-biplot}. We can cleary see that on the left upper corner we've got married people and on the other side we've got respectively divorced and single people which in general create group of the single people. On the opposite diagonal we have got encoded information about job and education. On the lower left corenr we've got correlated blue collar jobs and basic education and on the opposite side we've got administration jobs with university education. To our mind this is very interesting and exciting results as it was possible to so clearly distinguish groups of people.

```{r}
results_pca_df['job'] <- df_new_clients['job']
results_pca_df['marital'] <- df_new_clients['marital']
results_pca_df['education'] <- df_new_clients['education']
```

\begin{figure}
```{r fig.height=8, fig.width=5}
plot_marital <- results_pca_df %>% 
  ggplot(aes(x = PC1, y = PC2, color = marital)) +
  geom_point(size = 1)

plot_job <- results_pca_df %>% 
  ggplot(aes(x = PC1, y = PC2, color = job)) +
  geom_point(size = 1)

plot_education <- results_pca_df %>% 
  ggplot(aes(x = PC1, y = PC2, color = education)) +
  geom_point(size = 1)

plot_marital / plot_job / plot_education
```
\caption{Results of the Principal component analysis in the context of \text{marital}, \textit{job}, \textit{education} variable using age, job, marital, education, housing, loan variables.}
\label{fig:pca-ca-analysis}
\end{figure}

We checked these hypothesis on Figure \ref{fig:pca-ca-analysis}. We can clearly confirm our observation about marital status and seperate groups between three formulated lines. In the context of job we can also confirm that we have separate groups of administration and blue collar jobs. Moreover we can observe some kind of the structure in center clusters. At the last part of this plot we can observe some structure in education where on the bottom part we've got people with basic education, in the middle mixed but with majority of high school and professional courses, lastly we have at the top people with university degree.

At this point we will end up our PCA analysis. We've performed extensive analysis with very interesting results which will be definitely utilized during classification and cluster analysis. In context of the cluster analysis we could even say that we've partialy performed it with very satisfactory results.

\subsubsection{Multidimensional scaling}
In this section we will test another dimension reduction technique - multidimensional scaling. At this moment we recall that our dataset has `r nrow(df_new_clients)` rows. It's important in this moment because this method uses similarity matrix which has to compare all rows to all rows. In our case it would produce very big matrix which may be not possible to handle. Because of that fact we will use randomly selected 15% of our dataset with stratification on \textit{y} variable.

```{r}
df_new_client_yes <- df_new_clients %>% 
  filter(y == 'yes') %>% 
  sample_frac(0.15)

df_new_client_no <- df_new_clients %>% 
  filter(y == 'no') %>% 
  sample_frac(0.15)

df_new_clients_sample <- rbind.data.frame(df_new_client_no, df_new_client_yes)
```

```{r}
dissimilarities <- df_new_clients_sample %>% 
  dplyr::select(-y) %>% 
  mutate(
    job = factor(job),
    marital = factor(marital),
    education = factor(education),
    loan = factor(loan), 
    campaign = factor(campaign),
    contact_cellular = factor(contact_cellular)
  ) %>% 
  daisy() %>% 
  as.matrix()

mds <- cmdscale(dissimilarities)
```

\begin{figure}
```{r}
mds.df <- mds %>% data.frame()
mds.df <- bind_cols(mds.df, df_new_clients_sample)

mds.df %>% 
  ggplot(aes(x = X1, y = X2, color = y)) +
  geom_point()
```
\caption{Results of the Multidimensional scaling in context of \textit{y} variable.}
\label{fig:mds}
\end{figure}

We've performed dissimilarity calculation using gower metric without standarization (using standarization doesn't change anything) using all variables except \textit{y} and then used multidimensional scaling with rank equal 2. The results can be found on Figure \ref{fig:mds}. We can easily see two disjoint groups however unfortunately without clear separation on \textit{y}. Let's find out by which variables our transformed data was separated.

\begin{figure}
```{r fig.height=8, fig.width=5}
plot_housing <- mds.df %>% 
  ggplot(aes(x = X1, y = X2, color = housing)) +
  geom_point()

plot_loan <- mds.df %>% 
  ggplot(aes(x = X1, y = X2, color = loan)) +
  geom_point()

plot_euribor <- mds.df %>% 
  ggplot(aes(x = X1, y = X2, color = euribor3m)) +
  geom_point()

plot_housing / plot_loan / plot_euribor
```
\caption{Results of the Multidimensional scaling in the context of \text{housing}, \textit{loan}, \textit{euribor3m} variables.}
\label{fig:mds-housing-loan-euribor}
\end{figure}

We conclude from Figure \ref{fig:mds-housing-loan-euribor} that main splitting factor was housing. The other two the most interesting variables in our opinion was loan and euribor. Separation and specific areas of subgroups for the other variables also could be seen however not so clearly. Similarily to PCA example let's explore only client specific data.


```{r}
dissimilarities_client <- df_new_clients_sample %>% 
  dplyr::select(-y) %>% 
  mutate(
    job = factor(job),
    marital = factor(marital),
    education = factor(education),
    loan = factor(loan), 
    campaign = factor(campaign)
  ) %>% 
  dplyr::select(job, marital, education, housing, loan) %>% 
  daisy() %>% 
  as.matrix()

mds_client <- cmdscale(dissimilarities_client)
```

\begin{figure}
```{r fig.height=4, fig.width=8}
mds_client.df <- mds_client %>% data.frame()
mds_client.df <- bind_cols(mds_client.df, df_new_clients_sample)

mds_client.df %>% 
  ggplot(aes(x = X1, y = X2, color = y)) +
  geom_point()
```
\caption{Results of the Multidimensional scaling in context of \textit{y} variable for only client data.}
\label{fig:mds-client}
\end{figure}

We can observe results of the MDS in context of only client data on Figure \ref{fig:mds-client}. Also here we can find some separate groups. However this time we also cannot see any particular pattern in context of target variable. In next step we will try to find which variables separates groups.

\begin{figure}
```{r fig.height=4, fig.width=8}
mds_client.df %>% 
  mutate(
    housing = factor(housing),
    loan = factor(loan)
  ) %>% 
  ggplot(aes(x = X1, y = X2, color = housing, shape = loan)) +
  geom_point(size = 2, alpha = 0.7) + 
  scale_shape_manual(values = c(0, 4))
```
\caption{Results of the Multidimensional scaling in context of \textit{housing} and \textit{loan} variables for only client data.}
\label{fig:mds-client-separate}
\end{figure}

After quick search we've obtained the following results which are presented also on Figure \ref{fig:mds-client-separate}. Our main separating varialbe is \textit{housing} and in the group of clients which have a house we can also distinguish people with and without loan. It's quite interesting pattern because it's very natural separation in real-world as people can have house on credit / on their own or doesn't have a house and may or may not have a loan.

Concluding, multidimensional scaling gave us also very interesting results, especially in context of data visualization / understanding and grouping. It also may be very powerful tool for us in the next sections connected to classification and cluster analysis.

\subsection{Classification}
In this section we would like to evaluate if dimension reduction techniques could improve results in classification task. The whole set up will be the same as in previous report, i.e. AUCPR will be our main objective metric with 5-Fold Stratified Cross Validation. We will test the following configurations of data:
\begin{itemize}
  \item dataset with all columns with One Hot Encoding on \textit{job}, \textit{marital}, \textit{education} variables,
  \item first 17 columns of the PCA on the whole dataset,
  \item first 5 columns of the PCA on the whole dataset,
  \item first 2 columns of the PCA on the whole dataset
\end{itemize}
and we will try the following models:
\begin{itemize}
  \item Logistic Regression,
  \item Decision Tree,
  \item Random Forest (with 50 and 100 trees).
\end{itemize}

We will skip testing results obtained during multidimensional scaling as it's not possible to use whole dataset and the results between different transformations won't be comparable. Moreover we're testing different number of PCA components to check if we can compress our dataset while perserving similar accuracy. 

During the testing it turned out that our set up from first project was wrong because R treated `\textit{no} in target variables as {1} and because of that our results was so superior. In the following experiments we've fixed this bug.

```{r}
measures = list(
  msr("classif.precision", id = "precision_train", predict_sets = "train"),
  msr("classif.precision", id = "precision_test"),
  msr("classif.recall", id = "recall_train", predict_sets = "train"),
  msr("classif.recall", id = "recall_test"),
  msr("classif.prauc", id = "aucpr_train", predict_sets = "train"),
  msr("classif.prauc", id = "aucpr_test")
)

resamplings = rsmp("cv", folds = 5)
```

```{r}
df_modeling <- df_new_clients %>% 
  DataExplorer::dummify(select = c('job', 'marital', 'education'))

df_modeling_pca_17 <- results_pca_df %>% 
  dplyr::select(-PC18, -PC19, -PC20, -PC21, -PC22, -PC23)

df_modeling_pca_5 <- results_pca_df %>% 
  dplyr::select(PC1, PC2, PC3, PC4, PC5, y)

df_modeling_pca_2 <- results_pca_df %>% 
  dplyr::select(PC1, PC2, y)

task_dummified = TaskClassif$new(id = "standard", backend = df_modeling, target = 'y', positive = 'yes')
task_dummified$col_roles$stratum = 'y'

task_pca_17 = TaskClassif$new(id = "pca_17", backend = df_modeling_pca_17, target = 'y', positive = 'yes')
task_pca_17$col_roles$stratum = 'y'

task_pca_5 = TaskClassif$new(id = "pca_5", backend = df_modeling_pca_5, target = 'y', positive = 'yes')
task_pca_5$col_roles$stratum = 'y'

task_pca_2 = TaskClassif$new(id = "pca_2", backend = df_modeling_pca_2, target = 'y', positive = 'yes')
task_pca_2$col_roles$stratum = 'y'

tasks = c(
  task_dummified,
  task_pca_17,
  task_pca_5,
  task_pca_2
)
```

```{r}
learners = list(
  lrn("classif.log_reg", id = "log-reg", predict_type = "prob", predict_sets = c("train", "test")),
  lrn("classif.ranger", id = "decision-tree", num.trees = 1, predict_type = "prob", predict_sets = c("train", "test")),
  lrn("classif.ranger", id = "random-forest-50", num.trees = 50, predict_type = "prob", predict_sets = c("train", "test")),
  lrn("classif.ranger", id = "random-forest-100", num.trees = 100, predict_type = "prob", predict_sets = c("train", "test"))
)

design = benchmark_grid(tasks, learners, resamplings)

bmr = benchmark(design)
results_classification <- bmr$aggregate(measures)
```

```{r}
results_classification %>% 
  dplyr::select(-nr, -resample_result, -resampling_id, -iters) %>% 
  mutate(across(where(is.numeric), round, 10)) %>% 
  arrange(desc(aucpr_test)) %>% 
  xtable(
    caption = "Results of the classification accuracy of the models trained on dataset with and without PCA transformations.",
    label = "tab:classification"
  )
```

The results of the classification can be found in Table \ref{tab:classification}. It turned out that the best model is Random Forest with 100 trees on dataset without PCA transformation. The model looks very overfitted however we've set a lot of hyperparameters as defaults and it would be possible to obtain better results. We can also observe that almost all models trained on PCA transformed dataset were worse. Moreover the smaller number of dimensions of PCA transformation the worse results were. 

As we remember from our first report our dataset was very unbalanced with `r df_new_clients %>% filter(y == 'yes') %>% count() %>% pull()` \textit{yes} values and `r df_new_clients %>% filter(y == 'no') %>% count() %>% pull()` \textit{no} values. In the next experiment we will try undersampling and oversampling with ratio $\frac{1}{10}$ and $10$ respectively. We will also only try Logistic Regression and Random Forest with 100 trees as they were best performing and also we will only use whole dataset and transformed on PCA with first 17 components as lower number of components always performed worse.

```{r}
po_under = po("classbalancing", id = "undersample", adjust = "major", reference = "major", shuffle = FALSE, ratio = 1 / 10)
po_over = po("classbalancing", id = "oversample", adjust = "minor", reference = "minor", shuffle = FALSE, ratio = 10)

learners = list(
  lrn("classif.log_reg", id = "log-reg", predict_type = "prob"),
  lrn("classif.ranger", id = "random-forest-100", num.trees = 100, predict_type = "prob"),
  GraphLearner$new(po_under %>>% lrn("classif.log_reg", predict_type = "prob"), id = 'log.reg.under'),
  GraphLearner$new(po_over %>>% lrn("classif.log_reg", predict_type = "prob"), id = 'log.reg.over'),
  GraphLearner$new(po_under %>>% lrn("classif.ranger", num.trees = 100, predict_type = "prob"), id = 'rf.100.under'),
  GraphLearner$new(po_over %>>% lrn("classif.ranger", num.trees = 100, predict_type = "prob"), id = 'rf.100.over')
)

for (l in learners){
  l$predict_sets <- c("train", "test")
}

tasks = c(
  task_dummified,
  task_pca_17
)

design_balancing = benchmark_grid(tasks, learners, resamplings)

bmr_balancing = benchmark(design_balancing)
results_balancing <- bmr_balancing$aggregate(measures)
```


```{r}
results_balancing %>% 
  dplyr::select(-nr, -resample_result, -resampling_id, -iters) %>% 
  mutate(across(where(is.numeric), round, 10)) %>% 
  arrange(desc(aucpr_test)) %>% 
  xtable(
    caption = "Results of the classification accuracy of the models trained on dataset with and without PCA transformations with usage of undersampling and oversampling.",
    label = "tab:classification-balancing"
  )
```

The results of balancing experiment can be found in Table \ref{tab:classification-balancing}. Please note that \textit{ranger} in \textit{learner_id} is alias for Random Forest model with 100 trees. We can observe that also this time Random Forest on not transformed and unbalanced dataset was the best one. However the results of the undersampled Random Forest on not transformed dataset has quite close results in terms of AUCPR however it's not so overfitted. Unfortunately also this time the PCA transformation has not improved the results.

Summary...

\subsection{Clustering}
In this section we will focus on cluster analysis task. Our goal will be to group clients according to their similarity. We've done some part of the work in Dimension Reduction section as we were able to find some groups of clients after PCA and MDS transformations however in this part we will do that in more quantitative manner. We will test the following methods:
\begin{itemize}
  \item k-means,
  \item Partition Around Medoids (PAM),
  \item Aglomerative Nesting (AGNES)
\end{itemize}
and perform quality assesment of cluster analysis results. As we mentioned we want to group clients, so we will use only the features connected to clients, i.e.
 \begin{itemize}
  \item job,
  \item marital
  \item education,
  \item housing,
  \item loan.
\end{itemize}

\subsubsetction{K-means}
In this section we will perform cluster analysis using the K-means algorithm. We will begin our journey with this algorithm by looking for within-cluster and between-cluster dispersion depending on number of K. Then we will try to choose optimal number of clusters and analyse results for selected K. For this algorithm we also performed one hot encoding on our dataset.

```{r}
df_clustering <- df_new_clients %>% 
  dplyr::select(-y) %>% 
  dplyr::select(job, marital, education, housing, loan) %>% 
  DataExplorer::dummify()
```

```{r}
Within   <- c()
Between  <- c()
Total    <- c()

K.range <- 1:20

for (k in K.range)
{
  print(k)
  kmeans.k  <- kmeans(df_clustering, centers=k, iter.max=10, nstart=10)
  Within  <- c(Within, kmeans.k$tot.withinss)	# total within-cluster sum of squares =  sum(withinss))
  Between <- c(Between, kmeans.k$betweenss) # between-cluster sum of squares
  Total   <- c(Total,kmeans.k$totss) 	# total sum of squares.
  # remark: Total == Within + Between
}

y.range <- range(c(Within, Between, Total))

results_kmeans <- data.frame(
  K = K.range,
  Within = Within,
  Between = Between,
  Total = Total
) 
```

\begin{figure}
```{r fig.height=4, fig.width=8}
results_kmeans %>% 
  reshape2::melt(id.vars = c("K")) %>% 
  ggplot(aes(x = K, y = value, color = variable)) +
    geom_line() + 
    geom_point() +
    scale_x_discrete(limits = K.range) + 
    ylim(range(c(Within, Between, Total))) +
    scale_color_manual(values = c('red', 'blue', 'black')) +
    ggtitle("Comparison of the within-cluster and between-cluster dispersion")
```
\caption{Values of the within-cluster and between-cluster dispersion for different K in K-means algorithm.}
\label{fig:kmeans-dispersion}
\end{figure}

On the Figure \ref{fig:kmeans-dispersion} we can observe how does within-cluster and between-cluster changes depending on K. We can observe that we obtained standard elbow shape for both statistics however without a typical sharp value decrease in some point. Because of that we cannot easily choose optimal K. Let's try to find optimal value of K using silhouette statistic.

\begin{figure}
```{r fig.height=4, fig.width=8}
fviz_nbclust(df_clustering, FUNcluster = kmeans, method = "silhouette", k.max = 20)
```
\caption{Values of average silhouette width for different number of cluster K in K-means algorithm.}
\label{fig:kmeans-average-silhouette}
\end{figure}

The obtained results are presented on Figure \fig{fig:kmeans-average-silhouette}. Also this time we cannot easily select one best value as the values doesn't vary much. However we will select $K$ equal $13$ as the optimal one as it's very close to the best value $K = 18$ however it's a bit smaller value which enables us to more easily analyse the obtained clusters.

\begin{figure}
```{r fig.height=4, fig.width=8}
kmeans.k13 <- kmeans(df_clustering, centers=13, iter.max=15, nstart=10)

fviz_cluster(kmeans.k13, df_clustering)
```
\caption{Clusters obtained by K-means algorithm presented in 2D PCA space.}
\label{fig:kmeans-pca-space}
\end{figure}

In order to better understand our results we've plotted them in 2D PCA space on Figure \fig{fig:kmeans-pca-space}. We can observe some distinct groups even in this 2D visualization which fill us with hope that obtained results will be interpretable.

\begin{figure}
```{r fig.height=4, fig.width=8}
sil.kmeans <- silhouette(kmeans.k13$cluster, dist(df_clustering))
fviz_silhouette(sil.kmeans, xlab="K-means")
```
\caption{Clusters silhouette plot for K-means algorithm with K = 13.}
\label{fig:kmeans-silhouette}
\end{figure}

The results of the silhouette for individual clusters are presented on Figure \fig{fig:kmeans-silhouette}. We can observe that some of the clusters have very high silhouette values which means that they are properly assigned, but some of them have small results or even negative which suggests that some observations were incorrectly assigned. Especially the four negative peaks are probably the results of our choice of $13$ clusters instead of $18$.

Let us also compare the results in context of different categories and find some very homogenous groups. We will start with target variable \textit{y} however we don't expect that we will find some specific cluster with only \textit{yes} responses.

```{r results='asis'}
table(kmeans.k13$cluster, df_new_clients$y) %>% 
  xtable(
    caption = "Clusters vs. target variable",
    label = "tab:kmeans-cluster-vs-target"
  )
```

The results can be found in Table \ref{tab:kmeans-cluster-vs-target}. As we expected there is no clear separation between clients' responses.

In context of cluster interpretation we can extract one more useful information from Figure \ref{fig:kmeans-silhouette}. If cluster have very large silhouette it means that it should represent quite homogenuous group. Using that information we have found that
\begin{itemize}
  \item cluster 6 represents clients with basic education, married, blue-collar job and mostly without loan (which is also cluster found during visualization of PCA transformed data),
  \item cluster 13 represents clients with university degree, married, managment job and mostly without loan (which also was previously recognized),
  \item cluster 2 represents clients with universtiy degree, married, mostyle administration job and mostly with house,
  \item cluster 12 represents clients with professional course, mostly working as a technicians.
\end{itemize}

```{r}
table(kmeans.k13$cluster, df_new_clients$education)
```
```{r}
table(kmeans.k13$cluster, df_new_clients$marital)
```
```{r}
table(kmeans.k13$cluster, df_new_clients$job)
```
```{r}
table(kmeans.k13$cluster, df_new_clients$housing)
```
```{r}
table(kmeans.k13$cluster, df_new_clients$loan)
```

In our opinion these results give us very interesting insigth into our dataset and may be very helpful in process of better understanding of clients in bank.

\subsubsection{PAM}
In this section we will very similar analysis to the previous one on K-means however this time we will use another technique - Partition Around Medoids (PAM). This is more robust generalization of K-means in which we will use dissimilarity matrix with gower distance. This will allow us to encode categorical values in different way and we expect to obtain a little bit different results as we obtained different results for PCA and MDS. Because in this section we will use dissimilarity matrix we will perform stratified sampling the same way as we did in MDS.

```{r}
pam.silhouettes <- c()

K.range <- 2:15

for (k in K.range){
  print(k)
  pam.k <- pam(x = dissimilarities_client, k = k, diss = TRUE)
  sil.pam <- silhouette(pam.k$clustering, dissimilarities_client)
  pam.silhouettes <- c(pam.silhouettes, mean(sil.pam[, 3]))
}
```

\begin{figure}
```{r fig.height=4, fig.width=8}
data.frame(
  K = K.range,
  Silhouette = pam.silhouettes
) %>% 
  ggplot(aes(x = K, y = Silhouette)) +
    geom_line() + 
    geom_point() +
    scale_x_discrete(limits = K.range) + 
    ggtitle("Comparison of the average silhouette vs K for PAM algorithm.")
```
\caption{Values of average silhouette width for different number of cluster K in PAM algorithm.}
\label{fig:pam-average-silhouette}
\end{figure}

Firstly, we begin our analysis with finding optimal number of clusters. As we can observe on Figure \ref{fig:pam-average-silhouette} the optimal values is equal 13 - the same as we selected in K-means algorithm. Let's check how does silhouette looks for selected K.

\begin{figure}
```{r}
pam.k13 <- pam(x = dissimilarities_client, k = 13, diss = TRUE)
sil.pam13 <- silhouette(pam.k13$clustering, dissimilarities_client)
fviz_silhouette(sil.pam13, xlab="K-means")
```
\caption{Clusters silhouette plot for PAM algorithm with K = 13.}
\label{fig:pam-silhouette}
\end{figure}

The silhouette plot is presented on Figure \ref{fig:pam-silhouette}. Most of the obtained clusters are quite consistent in terms of silhouette statistics and in the next step we will explore them in more details.

```{r results='asis'}
table(pam.k13$clustering, df_new_clients_sample$y)
  xtable(
    caption = "Clusters vs. target variable",
    label = "tab:kmeans-cluster-vs-target"
  )
```

First of all, this time we also could find any structure in terms of target variable. Moreover we found that:
\begin{itemize}
  \item cluster 1 represents clients with university degree, married, administration / management / self-employed job with house and without loan,
  \item cluster 2 represents clients with basic education, mostly married and working as blue-collars with house and without loan,
  \item cluster 4 represents clients with mostly high school degree, married, working in services, with house and without loan,
  \item cluster 7 represents clients with university degree, single, administrative job, without house and without loan,
  \item cluster 9 represents clients with basic education, mostly married, working as blue-collars without house and without loan,
  \item cluster 13 represents clients with professional courses, mostly married, working as technicians, without housing and loans.
\end{itemize}
As we can observe the results are also very distinguishable however as we were analyzing the results we found that the main splitting criterion was housing and loans where for k-means it was mostly education, job and marital status. However it's quite natural as we used different type of measures for both algorithms. We have also seen differences between PCA and MDS representation and here we've corresponding results.

```{r}
table(pam.k13$clustering, df_new_clients_sample$education)
```
```{r}
table(pam.k13$clustering, df_new_clients_sample$marital)
```
```{r}
table(pam.k13$clustering, df_new_clients_sample$job)
```
```{r}
table(pam.k13$clustering, df_new_clients_sample$housing)
```
```{r}
table(pam.k13$clustering, df_new_clients_sample$loan)
```

\subsubsection{AGNES}


Objective: group objects according to their similarity
Methods: 
 - k-means
 - Partition Around Medoids (PAM)
 - Aglomerative Nesting (AGNES) 
 - Other (HDBSCAN?)
Quality assessment of cluster analysis results:
 - Average silhouette vs different K
 - Separation / Compactness / Connectedness
 
 Ideas:
  - Use Custering LARge Applications (CLARA) instead PAM if computations are slow
  - Different dissimilarity measures - Gower distance
  - Analyse what are the characteristic properties of objects thatwere assign to a given cluster (for example, in individualclusters you can analyse: average values for numerical featuresand counts for qualitative features).
  - Dendogram and banner plot for AGNES
  - Single / Complete / Average linkage for AGNES
  - Dunn index for cluster results analysis

Cluster results analysis:  
 - Internal validation:Use the averagesilhouette indexto comparethe results obtained for different clustering algorithms (e.g. PAMand AGNES) and for a different number of clustersK. Try to decideon the optimal number of clusters.
 - External validation:Use simple contingency table (confusionmatrix) to compare clustering results with real class membership. Compare results for different clustering algorithms, includingpartitioning and hierarchical methods. (Hint: you can use functionmatchClasses{e1071} to find anoptimal assignment (mapping) between two set of labels).
  
  Notes:
   - AGNES will probably require data sampling
  
\section{Results and discussion}
\label{sec:results}


\bibliographystyle{unsrt}
\bibliography{references}
